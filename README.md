# Local Ollama Project

Полноценная система локального искусственного интеллекта с веб-интерфейсом, работающая на вашем компьютере без подключения к интернету

## Особенности

- **Полная приватность** - все данные остаются на вашем компьютере
- **Память разговоров** - AI запоминает предыдущие сообщения в сессии
- **Доступ с любого устройства** - подключение через локальную сеть или интернет
- **Быстрая работа** - модель работает на вашем оборудовании
- **Современный интерфейс** - красивый и удобный веб-интерфейс

## Быстрый запуск

### 1. Запуск сервера
```bash
# Запустите файл start_server.bat
start_server.bat
```

### 2. Подключение
- **Локально**: http://localhost:5000
- **Из сети**: http://[ВАШ_IP]:5000

### 3. Получение IP адреса
```bash
powershell -ExecutionPolicy Bypass -File get_ip.ps1
```

## Системные требования

- Windows 10 / 11
- Python 3.8+
- Минимум 8 ГБ RAM (рекомендуется 16 ГБ)
- Свободное место: 10 ГБ для модели

## Установка компонентов

### Ollama
```bash
winget install Ollama.Ollama
```

### Python зависимости
```bash
pip install flask flask-cors requests python-dotenv
```

### Модель AI
```bash
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b
```

## Структура проекта

```
D:\local_ollama_project\
├── app.py                 # Основное Flask приложение
├── templates/
│   └── index.html         # Веб-интерфейс
├── chat_history.db        # База данных истории
├── start_server.bat       # Скрипт запуска
├── get_ip.ps1             # Получение IP адресов
└── requirements.txt       # Python зависимости
```

## Конфигурация

### Изменение модели
В файле `app.py` измените:
```python
MODEL_NAME = "llama3.1:8b"  # На вашу модель
```

### Доступные модели
- `llama3.1:8b` - Рекомендуемая (4.9 ГБ)
- `llama3.2:3b` - Быстрая (2.0 ГБ)
- `qwen2.5:7b` - Альтернативная
- `mistral:7b` - Другая опция

## Подключение с других устройств

### Подключение в локальной сети
- Устройства должны быть в одной Wi-Fi сети
- Брандмауэр должен разрешать подключения на порт 5000

### Настройка брандмауэра
```bash
# PowerShell от администратора
New-NetFirewallRule -DisplayName "AI Assistant" -Direction Inbound -Protocol TCP -LocalPort 5000 -Action Allow
```

### Подключение в локальной сети
1. Узнайте IP адрес сервера: `get_ip.ps1`
2. Откройте браузер на другом устройстве
3. Перейдите по адресу: `http://[IP_СЕРВЕРА]:5000`

## Использование

### Локальное использование
1. **Запустите сервер**: `start_server.bat`
2. **Откройте браузер**: http://localhost:5000
3. **Начните общение**: введите вопрос в поле ввода

### Подключение с других устройств в локальной сети
1. Узнайте IP адрес сервера: `get_ip.ps1`
2. Откройте браузер на другом устройстве
3. Перейдите по адресу: `http://[IP_СЕРВЕРА]:5000`

## База данных

История разговоров сохраняется в SQLite базе `chat_history.db`:
- Каждая сессия имеет уникальный ID
- Сохраняются сообщения пользователя и ответы AI
- История доступна в рамках сессии

## Безопасность

- Все данные обрабатываются локально
- Никакая информация не передается в интернет
- История разговоров хранится только на вашем компьютере
- Доступ только через локальную сеть

## Решение проблем

### Ollama не запускается
```bash
# Проверьте установку
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" --version

# Перезапустите сервис
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" serve
```

### Модель не загружается
```bash
# Проверьте доступные модели
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" list

# Загрузите модель заново
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b
```

### Не подключается с других устройств
1. Проверьте IP адрес: `get_ip.ps1`
2. Убедитесь что устройства в одной сети
3. Проверьте настройки брандмауэра
4. Попробуйте отключить антивирус временно

### Медленная работа
- Закройте другие программы
- Увеличьте объем RAM
- Используйте модель меньшего размера (llama3.2:3b)

## Поддержка

При возникновении проблем:
1. Проверьте логи в консоли
2. Убедитесь что все компоненты установлены
3. Проверьте доступность портов 11434 и 5000

## Обновление

Для обновления модели:
```bash
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b
```

Для обновления приложения:
- Замените файлы `app.py` и `templates/index.html`
- Перезапустите сервер

**Поздравляем! Ваш локальный AI ассистент готов к работе!**
