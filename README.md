# Voicaj LLM API Сервер

**Полноценная LLM модель на базе трансформеров** - оптимизирована для русского языка и структурированного JSON вывода согласно Voicaj Schema.

Локальный AI-ассистент со структурированным JSON выводом на основе собственной LLM модели. Работает на вашем компьютере без подключения к интернету и предоставляет интеллектуальную категоризацию пользовательского ввода для интеграции с iOS/mobile приложениями.

## Особенности

- **Гибридная архитектура** - Rule-based система для простых случаев + Neural Network для сложных
- **Интеллектуальная классификация** - Автоматическое определение типов задач и извлечение метаданных
- **Структурированный JSON вывод** - Строгое соответствие Voicaj LLM Schema
- **Русский язык** - Оптимизировано для работы с русским текстом
- **Двойной режим** - JSON Mode для структурированного вывода и обычный чат
- **История разговоров** - Сохранение в локальной SQLite базе данных
- **REST API** - Готов для интеграции с мобильными приложениями
- **Веб-интерфейс** - Удобный чат-интерфейс для тестирования
- **Полная приватность** - Все данные остаются на вашем компьютере
- **Система обучения** - Fine-tuning на основе обратной связи пользователей
- **Нейронные сети** - Использование Hugging Face Transformers для сложных случаев

## Быстрый запуск

### 1. Запуск сервера
```bash
# Запустите скрипт
.\start_server.bat
```

### 2. Подключение
- **Локально**: http://localhost:5000
- **Из сети**: http://[ВАШ_IP]:5000

### 3. Получение IP адреса
```bash
powershell -ExecutionPolicy Bypass -File get_ip.ps1
```

## Системные требования

- Windows 10 / 11
- Python 3.8+
- Минимум 8 ГБ RAM (рекомендуется 16 ГБ)
- Свободное место: 10 ГБ для модели

## Установка

### Требования
- Python 3.8+
- PyTorch (для работы с трансформерами)
- Transformers (Hugging Face)

### Установка зависимостей
```bash
# Установка всех необходимых пакетов
pip install -r requirements.txt

# Или установка по отдельности
pip install torch transformers flask requests numpy
```

### Первый запуск
```bash
# Запуск сервера
python app.py
```

## Архитектура

### Гибридная Voicaj LLM Model
- **Rule-based компонент**: Быстрая обработка простых запросов с высокой точностью
- **Neural Network компонент**: Microsoft DialoGPT для сложных случаев
- **Классификатор сложности**: Автоматический выбор между компонентами
- **Генератор JSON**: Создание структурированного вывода
- **Fine-tuning**: Оптимизация на основе обратной связи пользователей

### Компоненты системы
1. **hybrid_voicaj_llm.py** - Гибридная LLM система
2. **voicaj_llm.py** - Резервная rule-based система
3. **app.py** - Flask веб-сервер и API
4. **templates/index.html** - Веб-интерфейс
4. **chat_history.db** - SQLite база данных
5. **requirements.txt** - Зависимости Python

## Структура проекта

```
D:\local_ollama_project\
├── hybrid_voicaj_llm.py   # Гибридная LLM (Rule-based + Neural Network)
├── voicaj_llm.py          # Старая rule-based система (резерв)
├── voicaj_trainer.py      # Система обучения модели
├── voicaj_training_data.json # База примеров обучения
├── app.py                 # Flask веб-сервер и API
├── templates/
│   └── index.html         # Веб-интерфейс с JSON Mode и обратной связью
├── chat_history.db        # SQLite база данных
├── requirements.txt       # Python зависимости (включая transformers, torch)
├── start_server.bat      # Скрипт запуска
├── .gitignore            # Исключения для Git
└── README.md             # Документация
```

## Конфигурация

### Настройка модели
В файле `voicaj_llm.py` можно изменить:
- **Паттерны**: Добавить новые паттерны для распознавания типов задач
- **Теги**: Расширить базу тегов в `tags_keywords`
- **Обучение**: Добавить примеры в `voicaj_training_data.json`

### Настройка сервера
В файле `app.py`:
- **Порт**: `app.run(port=5000)`
- **Хост**: `app.run(host='0.0.0.0')` для доступа из сети

## Система обучения

Voicaj LLM поддерживает **Fine-tuning** на основе обратной связи пользователей:

### Как работает обучение:
1. **Пользователь** отправляет запрос и получает ответ модели
2. **Пользователь** нажимает кнопку "Обратная связь" в веб-интерфейсе
3. **Пользователь** описывает, что не понравилось в ответе
4. **Система** анализирует фидбек и генерирует улучшенный ответ
5. **Модель** сохраняет пример обучения для будущих ответов

### Типы улучшений:
- **Заголовки**: Более конкретные и описательные названия
- **Описания**: Детальные объяснения целей и контекста
- **Даты**: Правильное вычисление относительно текущего времени
- **Теги**: Точная категоризация по 16 категориям
- **Приоритеты**: Адекватная оценка важности задач

### База обучения:
- **9 примеров** покрывающих все сферы жизни
- **Автоматическое сохранение** в `voicaj_training_data.json`
- **Контекстное обучение** - модель использует похожие примеры

## Voicaj LLM Schema

API возвращает структурированный JSON на основе анализа пользовательского ввода:

### Поддерживаемые типы:
- **task** - Задачи, дедлайны, напоминания
- **diary_entry** - Личные заметки, размышления
- **habit** - Повторяющиеся действия, цели
- **health** - Показатели здоровья, сон, шаги
- **workout** - Физические активности, упражнения
- **meal** - Логирование еды, приемы пищи
- **goal** - Долгосрочные цели
- **advice** - Коучинг, запросы помощи
- **study_note** - Обучение, учебные материалы
- **time_log** - Учет времени
- **shared_task** - Совместные задачи
- **focus_session** - Pomodoro сессии
- **mood_entry** - Эмоциональное состояние
- **expense** - Финансовые транзакции
- **travel_plan** - Планирование путешествий

### Формат ответа API:
```json
{
  "response": {
    "type": "task",
    "title": "Завершить отчет по проекту",
    "description": "Завершить и отправить отчет по проекту",
    "priority": "high",
    "tags": ["работа", "дедлайн"],
    "dueDate": "2025-01-10 17:00",
    "address": null
  },
  "session_id": "uuid-string",
  "type": "structured_json"
}
```

## API Эндпоинты

### Основной чат эндпоинт
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Мне нужно завершить отчет по проекту к пятнице"
}
```

### Другие эндпоинты
- **Получить историю**: `GET /api/history`
- **Очистить историю**: `POST /api/clear`
- **Получить модели**: `GET /api/models`

## Интеграция с iOS

### Пример Swift:
```swift
struct VoicajResponse: Codable {
    let response: VoicajData
    let sessionId: String
    let type: String
    
    enum CodingKeys: String, CodingKey {
        case response
        case sessionId = "session_id"
        case type
    }
}

struct VoicajData: Codable {
    let type: String
    let title: String
    let description: String?
    let priority: String?
    let tags: [String]?
    let dueDate: String?
    let startDate: String?
    let timestamp: String?
}

func sendMessage(_ message: String) async throws -> VoicajResponse {
    let url = URL(string: "http://192.168.1.28:5000/api/chat")!
    var request = URLRequest(url: url)
    request.httpMethod = "POST"
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")
    
    let body = ["message": message]
    request.httpBody = try JSONSerialization.data(withJSONObject: body)
    
    let (data, _) = try await URLSession.shared.data(for: request)
    return try JSONDecoder().decode(VoicajResponse.self, from: data)
}
```

## Сетевой доступ

### Подключение в локальной сети
- Устройства должны быть в одной Wi-Fi сети
- Брандмауэр должен разрешать подключения на порт 5000

### Настройка брандмауэра
```bash
# PowerShell от администратора
New-NetFirewallRule -DisplayName "Voicaj AI Assistant" -Direction Inbound -Protocol TCP -LocalPort 5000 -Action Allow
```

### Подключение с других устройств
1. Получите IP сервера: `get_ip.ps1`
2. Откройте браузер на другом устройстве
3. Перейдите по адресу: `http://[IP_СЕРВЕРА]:5000`

## Использование

### Локальное использование
1. **Запустите сервер**: `.\start_server.bat`
2. **Откройте браузер**: http://localhost:5000
3. **Начните общение**: Введите сообщение в поле ввода

### Использование по сети
1. Получите IP сервера: `get_ip.ps1`
2. Откройте браузер на другом устройстве
3. Перейдите по адресу: `http://[IP_СЕРВЕРА]:5000`

## База данных

История разговоров сохраняется в SQLite базе `chat_history.db`:
- Каждая сессия имеет уникальный ID
- Сохраняются сообщения пользователя и ответы AI
- История доступна в рамках сессии

## Безопасность

- Все данные обрабатываются локально
- Никакая информация не передается в интернет
- История разговоров хранится только на вашем компьютере
- Доступ только через локальную сеть

## Решение проблем

### Ollama не запускается
```bash
# Проверьте установку
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" --version

# Перезапустите сервис
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" serve
```

### Модель не загружается
```bash
# Проверьте доступные модели
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" list

# Загрузите модель заново
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b
```

### Не подключается с других устройств
1. Проверьте IP адрес: `get_ip.ps1`
2. Убедитесь что устройства в одной сети
3. Проверьте настройки брандмауэра
4. Попробуйте отключить антивирус временно

### Медленная работа
- Закройте другие программы
- Увеличьте объем RAM
- Используйте модель меньшего размера (llama3.2:3b)

## Поддержка

При возникновении проблем:
1. Проверьте логи в консоли
2. Убедитесь что все компоненты установлены
3. Проверьте доступность портов 11434 и 5000

## Обновление

### Обновление модели
```bash
& "$env:LOCALAPPDATA\Programs\Ollama\ollama.exe" pull llama3.1:8b
```

### Обновление приложения
- Замените файлы `app.py` и `templates/index.html`
- Перезапустите сервер
